{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WEts3BsBIYWA"
      },
      "outputs": [],
      "source": [
        "# Q1. Implementing logistic regression using Python (both scratch and sk learn).\n",
        "\n",
        "# Importing numpy library\n",
        "import numpy as np\n",
        "\n",
        "#Logistic Regression from scatch\n",
        "\n",
        "class Logistic_Regression():\n",
        "\n",
        "  # declaring learning rate & number of iterations (Hyperparametes)\n",
        "  def __init__(self, learning_rate, no_of_iterations):\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "\n",
        "  # fit function to train the model with dataset\n",
        "  def fit(self, X, Y):\n",
        "\n",
        "    # number of data points in the dataset (number of rows)  -->  m\n",
        "    # number of input features in the dataset (number of columns)  --> n\n",
        "    self.m, self.n = X.shape\n",
        "\n",
        "  # initiating weight & bias value\n",
        "\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  # implementing Gradient Descent for Optimization\n",
        "\n",
        "    for i in range(self.no_of_iterations):\n",
        "     self.update_weights()\n",
        "\n",
        "  def update_weights(self):\n",
        "\n",
        "    # Y_cap formula (sigmoid function)\n",
        "    Y_cap = 1 / (1 + np.exp( - (self.X.dot(self.w) + self.b ) ))\n",
        "\n",
        "  # derivaties\n",
        "\n",
        "    dw = (1/self.m)*np.dot(self.X.T, (Y_cap - self.Y))\n",
        "    db = (1/self.m)*np.sum(Y_cap - self.Y)\n",
        "\n",
        "  # updating the weights & bias using gradient descent\n",
        "\n",
        "    self.w = self.w - self.learning_rate * dw\n",
        "    self.b = self.b - self.learning_rate * db\n",
        "\n",
        "  # Sigmoid Equation & Decision Boundary\n",
        "\n",
        "  def predict(self, X):\n",
        "    Y_pred = 1 / (1 + np.exp( - (X.dot(self.w) + self.b ) ))\n",
        "    Y_pred = np.where( Y_pred > 0.5, 1, 0)\n",
        "    return Y_pred\n",
        "\n",
        "\n",
        "# Logistic Regression using sklearn\n",
        "\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "  # Load the dataset\n",
        "    data = pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "\n",
        "  # Select features and target variable\n",
        "    X = data.iloc[:, [2, 3]].values  # Assuming columns 2 and 3 are the relevant features\n",
        "    y = data.iloc[:, 4].values  # Assuming column 4 is the target variable\n",
        "\n",
        "  # Split the dataset into training and testing sets (70:30 ratio)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=34)\n",
        "\n",
        "    model = LogisticRegression(random_state=34)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  # Make predictions\n",
        "    y_pred_sklearn = model.predict(X_test)\n",
        "\n",
        "  # Evaluate accuracy\n",
        "    accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "\n",
        "    print(\"Accuracy (Logistic Regression using scikit-learn): {:.2f}%\".format(accuracy_sklearn * 100))\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "  # Normalize the data\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_normalized = scaler.fit_transform(X_train)\n",
        "    X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "  # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_standardized = scaler.fit_transform(X_train)\n",
        "    X_test_standardized = scaler.transform(X_test)\n",
        "\n",
        "  # Logistic Regression using scikit-learn with normalized data\n",
        "    model_normalized = LogisticRegression(random_state=34)\n",
        "    model_normalized.fit(X_train_normalized, y_train)\n",
        "\n",
        "  # Logistic Regression using scikit-learn with standardized data\n",
        "    model_standardized = LogisticRegression(random_state=34)\n",
        "    model_standardized.fit(X_train_standardized, y_train)\n",
        "\n",
        "  # Make predictions\n",
        "    y_pred_normalized = model_normalized.predict(X_test_normalized)\n",
        "    y_pred_standardized = model_standardized.predict(X_test_standardized)\n",
        "\n",
        "  # Evaluate accuracy\n",
        "    accuracy_normalized = accuracy_score(y_test, y_pred_normalized)\n",
        "    accuracy_standardized = accuracy_score(y_test, y_pred_standardized)\n",
        "\n",
        "    print(\"Accuracy (Logistic Regression with Normalized Data): {:.2f}%\".format(accuracy_normalized * 100))\n",
        "    print(\"Accuracy (Logistic Regression with Standardized Data): {:.2f}%\".format(accuracy_standardized * 100))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.  Implementing k-NN using Python (both scratch and sk learn)\n",
        "\n",
        "# Implementing k-NN from scratch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X,y = iris.data, iris.target\n",
        "\n",
        "\n",
        "# Plot the dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
        "plt.xlabel('Sepal Length (cm)')\n",
        "plt.ylabel('Sepal Width (cm)')\n",
        "plt.title('Iris Dataset')\n",
        "plt.show()\n",
        "\n",
        "# Split the dataset into train and test sets (70:30 ratio)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    distance = np.sqrt(np.sum((x1-x2)**2))\n",
        "    return distance\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return predictions\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # compute the distance\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # get the closest k\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # majority voye\n",
        "        most_common = Counter(k_nearest_labels).most_common()\n",
        "        return most_common[0][0]\n",
        "\n",
        "\n",
        "# Implement k-NN using sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib.colors import ListedColormap\n",
        "from KNN import KNN\n",
        "\n",
        "k_values = range(1, 21)\n",
        "accuracy_scores_scratch = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "    y_pred_sklearn = knn_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred_sklearn)\n",
        "    accuracy_scores_sklearn.append(accuracy)\n",
        "\n",
        "# Plot accuracy vs k value\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_values, accuracy_scores_scratch, marker='o', label='k-NN (Scratch)')\n",
        "plt.plot(k_values, accuracy_scores_sklearn, marker='o', label='k-NN (scikit-learn)')\n",
        "plt.xlabel('k Value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs k Value for k-NN')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmV4FBvSiv8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}